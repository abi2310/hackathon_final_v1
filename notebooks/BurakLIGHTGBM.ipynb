{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c512d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Einen Ordner hochgehen (du bist in /notebooks, Modell liegt eine Ebene dar√ºber)\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# src/ zum Python-Pfad hinzuf√ºgen\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f2a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "import itertools\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2454b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spalten im DataFrame:\n",
      "['AuftragsID', 'BauteilID', 'Bauteilbezeichnung', 'Auftragseingang', 'Priorit√§t', 'Auftragsende_SOLL', 'Arbeitsschritt', 'Arbeitsschrittbezeichnung', 'AFO_Start_SOLL', 'AFO_Ende_SOLL', 'AFO_Start_IST', 'AFO_Ende_IST', 'MaschinenID', 'Maschinenbezeichnung', 'Auftragsende_IST']\n",
      "\n",
      "üîç Starte Tuning √ºber 192 Kombinationen...\n",
      "\n",
      "üîÅ Test 1/192\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 734\n",
      "[LightGBM] [Info] Number of data points in the train set: 914443, number of used features: 52\n",
      "[LightGBM] [Info] Start training from score 265.735938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burak1/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û°Ô∏è MAE 33.824 | n=350, leaves=64, min_child=20, subs=0.8, col=0.8\n",
      "üîÅ Test 2/192\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 734\n",
      "[LightGBM] [Info] Number of data points in the train set: 914443, number of used features: 52\n",
      "[LightGBM] [Info] Start training from score 265.735938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burak1/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û°Ô∏è MAE 33.487 | n=350, leaves=64, min_child=20, subs=0.8, col=0.9\n",
      "üîÅ Test 3/192\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 734\n",
      "[LightGBM] [Info] Number of data points in the train set: 914443, number of used features: 52\n",
      "[LightGBM] [Info] Start training from score 265.735938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burak1/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û°Ô∏è MAE 33.824 | n=350, leaves=64, min_child=20, subs=0.9, col=0.8\n",
      "üîÅ Test 4/192\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 734\n",
      "[LightGBM] [Info] Number of data points in the train set: 914443, number of used features: 52\n",
      "[LightGBM] [Info] Start training from score 265.735938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burak1/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û°Ô∏è MAE 33.487 | n=350, leaves=64, min_child=20, subs=0.9, col=0.9\n",
      "üîÅ Test 5/192\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 734\n",
      "[LightGBM] [Info] Number of data points in the train set: 914443, number of used features: 52\n",
      "[LightGBM] [Info] Start training from score 265.735938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burak1/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û°Ô∏è MAE 33.878 | n=350, leaves=64, min_child=40, subs=0.8, col=0.8\n",
      "üîÅ Test 6/192\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 734\n",
      "[LightGBM] [Info] Number of data points in the train set: 914443, number of used features: 52\n",
      "[LightGBM] [Info] Start training from score 265.735938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burak1/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û°Ô∏è MAE 33.061 | n=350, leaves=64, min_child=40, subs=0.8, col=0.9\n",
      "üîÅ Test 7/192\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014482 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 734\n",
      "[LightGBM] [Info] Number of data points in the train set: 914443, number of used features: 52\n",
      "[LightGBM] [Info] Start training from score 265.735938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burak1/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û°Ô∏è MAE 33.878 | n=350, leaves=64, min_child=40, subs=0.9, col=0.8\n",
      "üîÅ Test 8/192\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 734\n",
      "[LightGBM] [Info] Number of data points in the train set: 914443, number of used features: 52\n",
      "[LightGBM] [Info] Start training from score 265.735938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burak1/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û°Ô∏è MAE 33.061 | n=350, leaves=64, min_child=40, subs=0.9, col=0.9\n",
      "üîÅ Test 9/192\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014447 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 734\n",
      "[LightGBM] [Info] Number of data points in the train set: 914443, number of used features: 52\n",
      "[LightGBM] [Info] Start training from score 265.735938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burak1/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û°Ô∏è MAE 33.833 | n=350, leaves=64, min_child=60, subs=0.8, col=0.8\n",
      "üîÅ Test 10/192\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 734\n",
      "[LightGBM] [Info] Number of data points in the train set: 914443, number of used features: 52\n",
      "[LightGBM] [Info] Start training from score 265.735938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 143\u001b[0m\n\u001b[1;32m    127\u001b[0m model \u001b[38;5;241m=\u001b[39m LGBMRegressor(\n\u001b[1;32m    128\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39mN,\n\u001b[1;32m    129\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m    136\u001b[0m )\n\u001b[1;32m    138\u001b[0m pipe_tmp \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m    139\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprep\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocessor),\n\u001b[1;32m    140\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, model)\n\u001b[1;32m    141\u001b[0m ])\n\u001b[0;32m--> 143\u001b[0m \u001b[43mpipe_tmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m preds \u001b[38;5;241m=\u001b[39m pipe_tmp\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m    145\u001b[0m mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_test, preds)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/pipeline.py:662\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    657\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    658\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    659\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[1;32m    660\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    661\u001b[0m         )\n\u001b[0;32m--> 662\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/lightgbm/sklearn.py:1398\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   1382\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1383\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1395\u001b[0m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1396\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBMRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1398\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/lightgbm/sklearn.py:1049\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1046\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1047\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m-> 1049\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mnum_feature()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/lightgbm/engine.py:322\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    311\u001b[0m     cb(\n\u001b[1;32m    312\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[1;32m    313\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    319\u001b[0m         )\n\u001b[1;32m    320\u001b[0m     )\n\u001b[0;32m--> 322\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/lightgbm/basic.py:4155\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   4153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4154\u001b[0m _safe_call(\n\u001b[0;32m-> 4155\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4159\u001b[0m )\n\u001b[1;32m   4160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   4161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# üî• FULL WORKING LIGHTGBM TUNING PIPELINE üî•\n",
    "# Kein Preprocessor n√∂tig ‚Äì alles hier drin.\n",
    "# ============================================\n",
    "\n",
    "from src.data.load_data import load_data\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# =============================\n",
    "# Konstanten\n",
    "# =============================\n",
    "TARGET_END = \"Auftragsende_IST\"\n",
    "START_COL = \"Auftragseingang\"\n",
    "DATE_COLS = [\n",
    "    \"Auftragseingang\",\n",
    "    \"Auftragsende_SOLL\",\n",
    "    \"AFO_Start_SOLL\",\n",
    "    \"AFO_Ende_SOLL\",\n",
    "    \"AFO_Start_IST\",\n",
    "    \"AFO_Ende_IST\",\n",
    "]\n",
    "\n",
    "# =============================\n",
    "# 1) Daten laden\n",
    "# =============================\n",
    "data = load_data()\n",
    "\n",
    "# Datumsfelder ‚Üí Datetime\n",
    "for col in DATE_COLS + [TARGET_END]:\n",
    "    data[col] = pd.to_datetime(data[col], errors=\"coerce\")\n",
    "\n",
    "# g√ºltige rows filtern\n",
    "mask = (~data[TARGET_END].isna()) & (~data[START_COL].isna())\n",
    "data = data[mask].copy()\n",
    "\n",
    "start_dt = data[START_COL]\n",
    "\n",
    "# Dauer in Tagen als Target\n",
    "y = (data[TARGET_END] - start_dt).dt.total_seconds() / 86400.0\n",
    "y = y.astype(\"float32\")\n",
    "\n",
    "# =============================\n",
    "# 2) Date-Features erstellen\n",
    "# =============================\n",
    "for col in DATE_COLS:\n",
    "    data[f\"{col}_dow\"] = data[col].dt.dayofweek\n",
    "    data[f\"{col}_hour\"] = data[col].dt.hour\n",
    "    data[f\"{col}_day\"]  = data[col].dt.day\n",
    "    data[f\"{col}_month\"] = data[col].dt.month\n",
    "    data[f\"{col}_week\"] = data[col].dt.isocalendar().week.astype(int)\n",
    "\n",
    "# Originaldatum remove\n",
    "data = data.drop(columns=DATE_COLS + [TARGET_END])\n",
    "\n",
    "# IDs entfernen\n",
    "for col in [\"AuftragsID\", \"BauteilID\", \"MaschinenID\"]:\n",
    "    if col in data.columns:\n",
    "        data = data.drop(columns=[col])\n",
    "\n",
    "# =============================\n",
    "# 3) Preprocessor definieren\n",
    "# =============================\n",
    "categorical = data.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric     = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]), categorical),\n",
    "\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        ]), numeric),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# 4) Train/Test Split\n",
    "# =============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, y, test_size=0.35, random_state=32\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# 5) GRID-TUNING\n",
    "# =============================\n",
    "param_grid = {\n",
    "    \"n_estimators\": [350, 470],         # 2 Werte\n",
    "    \"num_leaves\": [96, 128, 256],       # 3 Werte\n",
    "    \"min_child_samples\": [30, 60],      # 2 Werte\n",
    "    \"subsample\": [0.85, 0.9],           # 2 Werte\n",
    "    \"colsample_bytree\": [0.85, 0.9],    # 2 Werte\n",
    "}\n",
    "\n",
    "combos = list(itertools.product(\n",
    "    param_grid[\"n_estimators\"],\n",
    "    param_grid[\"num_leaves\"],\n",
    "    param_grid[\"min_child_samples\"],\n",
    "    param_grid[\"subsample\"],\n",
    "    param_grid[\"colsample_bytree\"],\n",
    "))\n",
    "\n",
    "print(f\"\\nüîç Starte Tuning √ºber {len(combos)} Kombinationen...\\n\")\n",
    "\n",
    "best_mae = 99999\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "for i, (N, L, M, SUB, COL) in enumerate(combos, 1):\n",
    "\n",
    "    print(f\"üîÅ Test {i}/{len(combos)}\")\n",
    "\n",
    "    model = LGBMRegressor(\n",
    "        n_estimators=N,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=L,\n",
    "        min_child_samples=M,\n",
    "        subsample=SUB,\n",
    "        colsample_bytree=COL,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    pipe_tmp = Pipeline([\n",
    "        (\"prep\", preprocessor),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    pipe_tmp.fit(X_train, y_train)\n",
    "    preds = pipe_tmp.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "\n",
    "    print(f\"‚û°Ô∏è MAE {mae:.3f} | n={N}, leaves={L}, min_child={M}, subs={SUB}, col={COL}\")\n",
    "\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        best_params = (N, L, M, SUB, COL)\n",
    "        best_model = pipe_tmp\n",
    "\n",
    "# =============================\n",
    "# 6) Ergebnis\n",
    "# =============================\n",
    "print(\"\\n==============================\")\n",
    "print(\"üèÜ BESTE PARAMETER GEFUNDEN\")\n",
    "print(\"==============================\")\n",
    "print(\"MAE:\", best_mae)\n",
    "print(\"n_estimators:\", best_params[0])\n",
    "print(\"num_leaves:\", best_params[1])\n",
    "print(\"min_child_samples:\", best_params[2])\n",
    "print(\"subsample:\", best_params[3])\n",
    "print(\"colsample_bytree:\", best_params[4])\n",
    "\n",
    "# Speichern\n",
    "os.makedirs(\"models/lightgbm/pipeline\", exist_ok=True)\n",
    "joblib.dump(best_model, \"models/lightgbm/pipeline/best_lgbm_pipeline.pkl\")\n",
    "\n",
    "print(\"\\nüì¶ Modells gespeichert!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
